{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd1f40e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on the reviews against Jio network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfc7d8",
   "metadata": {},
   "source": [
    "In this project, we have aimed to analysis the reviews on the Jio network operator and classify them accordingly into satisfaction level as \"Satisfied\", \"Unsatisfied\" and \"Neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b9c24",
   "metadata": {},
   "source": [
    "### Link for the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97647bd5",
   "metadata": {},
   "source": [
    "https://drive.google.com/drive/folders/1WJvYR2JOuJ4FD4KAc4e3DJjPYZUanOV5?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc1500",
   "metadata": {},
   "source": [
    "## Importing all required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f4b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install seaborn\n",
    "# pip install numpy\n",
    "# pip install pandas\n",
    "# pip install mlxtend.preprocessin\n",
    "# pip install scipy.stats\n",
    "# pip install geopy.geocoders\n",
    "# pip install mpl_toolkits.basemap\n",
    "# pip install os\n",
    "# pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d764f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "from scipy.stats import norm\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Assgn1-Data-111903129_111903130_111903131_old.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9b54e",
   "metadata": {},
   "source": [
    "## Scrapped Data Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5065ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883410e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e2772",
   "metadata": {},
   "source": [
    "## Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ebc50",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41313d",
   "metadata": {},
   "source": [
    "## Instance of Geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0eb774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "loc = Nominatim(user_agent=\"GetLoc\")\n",
    "getLoc = loc.geocode(\"Mumbai,India\")\n",
    "print(getLoc.latitude,getLoc.longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab2e58b",
   "metadata": {},
   "source": [
    "# Updated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Assign1-Data-111903129_111903130_111903131.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdd928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5338fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee016c5",
   "metadata": {},
   "source": [
    "# Heatmap of the Co-relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c82ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26354f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataplot = sns.heatmap(df.corr() , cmap=\"YlGnBu\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c523bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(df,hue='Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c7b0fa",
   "metadata": {},
   "source": [
    "## Geo-spatial Mapping of the reviewer's location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d750c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20, 15))\n",
    "map = Basemap(width=1200000,height=900000,projection='lcc',resolution='l',\n",
    "                    llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n",
    "\n",
    "map.drawmapboundary ()\n",
    "map.drawcountries ()\n",
    "map.drawcoastlines ()\n",
    "\n",
    "lg=array(df['Longitude'])\n",
    "lt=array(df['Latitude'])\n",
    "\n",
    "\n",
    "x, y = map(lg, lt)\n",
    "plt.scatter(x, y, marker=\"o\", cmap=cm.Dark2, alpha=0.7)\n",
    "plt.title('JIO NETWORK REVIEWS SCATTERPLOT',fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7df68",
   "metadata": {},
   "source": [
    "# Outliers Removed Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66caa69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv(\"Outlier_removed_year.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20, 15))\n",
    "map = Basemap(width=1200000,height=900000,projection='lcc',resolution='l',\n",
    "                    llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n",
    "\n",
    "map.drawmapboundary ()\n",
    "map.drawcountries ()\n",
    "map.drawcoastlines ()\n",
    "\n",
    "lg=array(mydata['Longitude'])\n",
    "lt=array(mydata['Latitude'])\n",
    "\n",
    "\n",
    "x, y = map(lg, lt)\n",
    "plt.scatter(x, y, marker=\"o\", cmap=cm.Dark2, alpha=0.7)\n",
    "plt.title('JIO NETWORK REVIEWS SCATTERPLOT',fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafdc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "data=mydata[\"State\"]\n",
    "encoder=ce.OneHotEncoder(cols='State',handle_unknown='return_nan',return_df=True,use_cat_names=True)\n",
    "\n",
    "#Original Data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179697e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = encoder.fit_transform(data)\n",
    "data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc35459",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [15, 7]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, ax = plt.subplots()\n",
    "df = pd.DataFrame({'States': data})\n",
    "df['States'].value_counts().plot(ax=ax, kind='bar', xlabel='States', ylabel='Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a2d8a",
   "metadata": {},
   "source": [
    "# Reviews from different Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to plot\n",
    "labels = \"Central\",\"East\",\"West\",\"South\",\"North\"\n",
    "#North = 2607\n",
    "#south 1606\n",
    "#central 3965\n",
    "#west 1791\n",
    "#east 1890\n",
    "sizes = [3965,1890,1791,1606,2607]\n",
    "colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue','red']\n",
    "explode = (0.2, 0, 0, 0,0)  # explode 1st slice\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=True)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f71316",
   "metadata": {},
   "source": [
    "# PreProcessing on Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7baacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('Outlier_removed_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbfc4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text):\n",
    "# Removes all special characters and numericals leaving the alphabets\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "    return text\n",
    "\n",
    "# Cleaning the text in the review column\n",
    "X = mydata['Comments'].apply(clean)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36295951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.apply(token_stop_pos)\n",
    "# X.head()\n",
    "\n",
    "X = X[0:1].apply(token_stop_pos)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911aacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.apply(lemmatize)\n",
    "# X.head()\n",
    "print(X.apply(lemmatize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b372d8",
   "metadata": {},
   "source": [
    "## Saving the Lemmatized data to reduce Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mydata.to_csv('finalize.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lemmatize = pd.read_csv('https://drive.google.com/uc?export=download&id=1LLBtx9seVzpYIPH39-uJNKfiP_RLIRGJ')\n",
    "Lemmatize.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c0146",
   "metadata": {},
   "source": [
    "# Hashing(Word Level) the lemmatize Comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c98380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f2d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Lemmatize[\"Lemma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7955e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(X)\n",
    "x =x[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f898f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle(text: str):\n",
    "    shingle_set = text.split()\n",
    "    return set(shingle_set)\n",
    "\n",
    "\n",
    "b = set()\n",
    "for row in x:\n",
    "    a = shingle(row)\n",
    "    b = b.union(a)\n",
    "    \n",
    "limit= 10\n",
    "b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42871c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_list = []\n",
    "\n",
    "for row in x:\n",
    "    h1 = [1 if x in row else 0 for x in b]\n",
    "    hot_list.append(h1)\n",
    "signature = []\n",
    "\n",
    "print(hot_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "def create_hash_func(size: int):\n",
    "    # function for creating the hash vector/function\n",
    "    hash_ex = list(range(1, len(b)+1))\n",
    "    shuffle(hash_ex)\n",
    "    return hash_ex\n",
    "\n",
    "def build_minhash_func(vocab_size: int, nbits: int):\n",
    "    # function for building multiple minhash vectors\n",
    "    hashes = []\n",
    "    for _ in range(nbits):\n",
    "        hashes.append(create_hash_func(vocab_size))\n",
    "    return hashes\n",
    "\n",
    "\n",
    "def create_hash(vector: list):\n",
    "    # use this function for creating our signatures (eg the matching)\n",
    "    signature = []\n",
    "    for func in minhash_func:\n",
    "        for i in range(1, len(b)+1):\n",
    "            idx = func.index(i)\n",
    "            signature_val = vector[idx]\n",
    "            if signature_val == 1:\n",
    "                signature.append(i)\n",
    "                break\n",
    "    return signature\n",
    "\n",
    "# we create 20 minhash vectors\n",
    "minhash_func = build_minhash_func(len(b), 20)\n",
    "print(len(minhash_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e6b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_matrix = []\n",
    "for i in range(len(hot_list)):\n",
    "    signature_matrix.append(create_hash(hot_list[i]))\n",
    "\n",
    "print('--------------------------------------------------------------------------------')\n",
    "def matrix_print(m):\n",
    "    for i in range(len(m)):\n",
    "        if(i <limit):\n",
    "            print(\"\\t\\t\\t\",end=\"\")\n",
    "            print(m[i])\n",
    "    \n",
    "print(\"Signature Matrix :\")\n",
    "matrix_print(signature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6efadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(a: set, b: set):\n",
    "    return len(a.intersection(b)) / len(a.union(b))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Jaccard Similarity : \\n\")\n",
    "a = x[0]\n",
    "for i in range(len(hot_list)):\n",
    "    a = jaccard(set(signature_matrix[0]),set(signature_matrix[i]))\n",
    "    if(i < limit):\n",
    "        print(f\"\\t\\tJaccard similarity of comment1 with comment {(i+1)} :\", round(a,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f68f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a,b):\n",
    "    a = a.split(\" \")\n",
    "    bb = b.split(\" \")\n",
    "    dicc = {}\n",
    "    c = set(a)\n",
    "    d = set(b)\n",
    "    c = c.union(d)\n",
    "    for i in a :\n",
    "        if i in dicc:\n",
    "            a = dicc[i][0]\n",
    "            b = dicc[i][1]\n",
    "            dicc[i] = [a+1,b]\n",
    "        else :\n",
    "            dicc[i] = [1,0]\n",
    "    for i in bb:\n",
    "        if i in dicc:\n",
    "            a = dicc[i][0]\n",
    "            b = dicc[i][1]\n",
    "            dicc[i] = [a,b+1]\n",
    "        else :\n",
    "            dicc[i] = [0,1]\n",
    "\n",
    "    temp = dicc.keys()\n",
    "    sum_c = 0\n",
    "    sum_a = 0 \n",
    "    sum_b = 0\n",
    "    for i in temp:\n",
    "        k = dicc[i]\n",
    "        sum_c += (k[0]*k[1])\n",
    "        sum_a += (k[0] * k[0])\n",
    "        sum_b += ( k[1] * k[1])\n",
    "\n",
    "    return ( sum_c / ( sum_a**(1/2) * sum_b**(1/2)) )\n",
    "print(\"Cosine Similarity \")\n",
    "for i in range(len(hot_list)):\n",
    "    a = cosine(x[0],x[i])\n",
    "    if(i < limit):\n",
    "        print(f\"\\t\\tcosine similarity of comment1 with comment {(i+1)} :\", round(a,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38712134",
   "metadata": {},
   "outputs": [],
   "source": [
    "band = 20\n",
    "row = 5\n",
    "\n",
    "lsh_matrix =[]\n",
    "\n",
    "def split_vector(signature, b):\n",
    "    assert len(signature) % b == 0\n",
    "    r = int(len(signature) / b)\n",
    "    # code splitting signature in b parts\n",
    "    subvecs = []\n",
    "    for i in range(0, len(signature), r):\n",
    "        subvecs.append(signature[i : i+r])\n",
    "    return subvecs\n",
    "\n",
    "for i in range(len(hot_list)):\n",
    "    a = split_vector(signature_matrix[i],band)\n",
    "    lsh_matrix.append(a)\n",
    "print(\"LSH Matrix :\")\n",
    "def th_matrix_print(m):\n",
    "    for i in range(len(m)):\n",
    "        if(  i <  limit):\n",
    "            print(\"\\t\\t\\t\",end=\"\")\n",
    "            print(m[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4193938",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_matrix_print(lsh_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53dc310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "class LSH:\n",
    "    buckets = []\n",
    "    counter = 0\n",
    "    def __init__(self, b):\n",
    "        self.b = b\n",
    "        for i in range(b):\n",
    "            self.buckets.append({})\n",
    "\n",
    "    def make_subvecs(self, signature):\n",
    "        l = len(signature)\n",
    "        assert l % self.b == 0\n",
    "        r = int(l / self.b)\n",
    "        # break signature into subvectors\n",
    "        subvecs = []\n",
    "        for i in range(0, l, r):\n",
    "            subvecs.append(signature[i:i+r])\n",
    "        return np.stack(subvecs)\n",
    "    \n",
    "    def add_hash(self, signature):\n",
    "        subvecs = self.make_subvecs(signature).astype(str)\n",
    "        for i, subvec in enumerate(subvecs):\n",
    "            subvec = ','.join(subvec)\n",
    "            if subvec not in self.buckets[i].keys():\n",
    "                self.buckets[i][subvec] = []\n",
    "            self.buckets[i][subvec].append(self.counter)\n",
    "        self.counter += 1\n",
    "\n",
    "    def check_candidates(self):\n",
    "        candidates = []\n",
    "        for bucket_band in self.buckets:\n",
    "            keys = bucket_band.keys()\n",
    "            for bucket in keys:\n",
    "                hits = bucket_band[bucket]\n",
    "                if len(hits) > 1:\n",
    "                    candidates.extend(combinations(hits, 2))\n",
    "        return set(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "band = 20 \n",
    "row = 5\n",
    "\n",
    "lsh_matrix =[]\n",
    "\n",
    "def split_vector(signature, b):\n",
    "    assert len(signature) % b == 0\n",
    "    r = int(len(signature) / b)\n",
    "    # code splitting signature in b parts\n",
    "    subvecs = []\n",
    "    for i in range(0, len(signature), r):\n",
    "        subvecs.append(signature[i : i+r])\n",
    "    return subvecs\n",
    "\n",
    "for i in range(len(hot_list)):\n",
    "    a = split_vector(signature_matrix[i],band)\n",
    "    lsh_matrix.append(a)\n",
    "print(\"LSH Matrix :\")\n",
    "def th_matrix_print(m):\n",
    "    for i in range(len(m)):\n",
    "        if(  i <  limit):\n",
    "            print(\"\\t\\t\\t\",end=\"\")\n",
    "            print(m[i])\n",
    "\n",
    "    \n",
    "th_matrix_print(lsh_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222dd350",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 20\n",
    "\n",
    "lsh = LSH(b)\n",
    "\n",
    "for signature in signature_matrix:\n",
    "    lsh.add_hash(signature)\n",
    "print(\"Buckets:\")\n",
    "candidate_pairs = lsh.check_candidates()\n",
    "th_matrix_print(lsh.buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame({\n",
    "    'x': [],\n",
    "    'y': [],\n",
    "    'jaccard': [],\n",
    "    'cosine': [],\n",
    "    'candidate': []\n",
    "})\n",
    "\n",
    "for i in range(len(hot_list)):\n",
    "    for j in range(i,len(hot_list)):\n",
    "        candidate = 1 if (i,j) in candidate_pairs else 0\n",
    "        pairs = pairs.append({\n",
    "            'x': i,\n",
    "            'y': j,\n",
    "            'jaccard': jaccard(set(signature_matrix[i]), set(signature_matrix[j])),\n",
    "            'cosine': cosine(x[i],x[j]),\n",
    "            'candidate': candidate\n",
    "        },ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c798d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_min = pairs['cosine'].min()\n",
    "cos_max = pairs['cosine'].max()\n",
    "pairs['cosine_norm'] = (pairs['cosine'] - cos_min) / (cos_max - cos_min)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=pairs, x='cosine', y='candidate', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd80354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(s, r, b):\n",
    "    # s: similarity\n",
    "    # r: rows (per band)\n",
    "    # b: number of bands\n",
    "    return 1 - (1 - s**r)**b\n",
    "\n",
    "def normalize(x, x_min, x_max):\n",
    "    return (x - x_min) / (x_max - x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = pd.DataFrame({\n",
    "    'P': [],\n",
    "    's': [],\n",
    "    'b': []\n",
    "})\n",
    "\n",
    "for b in [50, 25,20, 15,10,5]:\n",
    "    r = int(100/b)\n",
    "    s_scores = np.arange(0.01, 1, 0.01)\n",
    "    P_scores = [probability(s, r, b) for s in s_scores]\n",
    "    probs = probs.append(pd.DataFrame({\n",
    "        'P': P_scores,\n",
    "        's': s_scores,\n",
    "        'b': [str(b)]*len(s_scores)\n",
    "    }), ignore_index=True)\n",
    "\n",
    "sns.lineplot(data=probs, x='s', y='P', hue='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in [50, 25,20, 15,10,5]:\n",
    "    r = int(100/b)    \n",
    "    print(\"Threshold for band \",b,\"and row \",r,\"is given as\", (1/b)**(1/r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4daecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 20\n",
    "r = 5\n",
    "s_scores = np.arange(0.01, 1, 0.01)\n",
    "P_scores = [probability(s, r, b) for s in s_scores]\n",
    "\n",
    "graph = sns.lineplot(x=s_scores, y=P_scores)\n",
    "graph = sns.scatterplot(data=pairs, x='cosine', y='candidate', alpha=0.1, color='k')\n",
    "graph.axhline((1/b)**(1/r), color='red')\n",
    "# graph.axvline((1/b)**(1/r), color='red',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199e621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc29b769",
   "metadata": {},
   "source": [
    "# Vader Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c4aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment\n",
    "Lemmatize.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# function to calculate vader sentiment  \n",
    "def vadersentimentanalysis(review):\n",
    "    vs = analyzer.polarity_scores(review)\n",
    "    return vs['compound']\n",
    "\n",
    "\n",
    "def vader_analysis(compound):\n",
    "    if compound >= 0.5:\n",
    "        return 1\n",
    "    elif compound <= -0.5 :\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "mydata['Vader Sentiment'] = Lemmatize['Lemma'].apply(vadersentimentanalysis)\n",
    "mydata['VaderAnalysis'] = mydata['Vader Sentiment'].apply(vader_analysis)\n",
    "mydata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1496b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba93e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_counts = mydata[\"VaderAnalysis\"].value_counts()\n",
    "tb_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61ba67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tb_count= mydata[\"VaderAnalysis\"].value_counts()\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.pie(tb_counts.values, labels = [\"Satisfied\",\"Neutral\",\"Unsatisfied\"], explode = (0.15, 0, 0.25), autopct='%1.1f%%', shadow=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd64fb3",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01587c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mydata[\"stars\"]\n",
    "Y = []\n",
    "column = list(y)\n",
    "for i in column:\n",
    "    if i < 3:\n",
    "        Y.append(-1)\n",
    "    elif i == 3:\n",
    "        Y.append(0)\n",
    "    else:\n",
    "        Y.append(1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141cc7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Y\n",
    "X = Lemmatize[\"Lemma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d41976",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range = (1,1), max_df = .95, min_df = 10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b10351",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train) \n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(X_train_dtm, y_train)\n",
    "y_pred = LR.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e853f76e",
   "metadata": {},
   "source": [
    "## Performace Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfb4cf",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d332623",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy Score: ',metrics.accuracy_score(y_test,y_pred)*100,'%',sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17369e9f",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4854a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix: ',metrics.confusion_matrix(y_test,y_pred), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2db6dd",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall Score: \", metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258cf76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    text = clean(text)\n",
    "    text = pd.DataFrame([text])\n",
    "    text = text[0].apply(token_stop_pos)\n",
    "    text = text.apply(lemmatize)\n",
    "    text_dtm = vect.transform(text)\n",
    "    pred = LR.predict(text_dtm)\n",
    "#     print(pred)\n",
    "    if pred[0] == 1 :\n",
    "        return \"Customer is Satisfied\"\n",
    "    if pred[0] == 0:\n",
    "        return \"Customer Comment is Neutral\"\n",
    "    if pred[0] == -1 :\n",
    "        return \"Customer is Unsatisfied\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c74469",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Jio is fairly good\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Jio network speed is mediocre\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a7b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Jio is worst\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c8895",
   "metadata": {},
   "source": [
    "# Year Wise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aac32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_summary = pd.read_csv('Outlier_removed.csv', usecols=['Year','stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataYear = year_summary['Year']\n",
    "dataStar = year_summary['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly = {}\n",
    "yearLabels = [2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
    "for i in range(len(year_summary)):\n",
    "    if dataYear[i] in yearly:\n",
    "        if dataStar[i] > 3:\n",
    "            yearly[dataYear[i]][0] +=1\n",
    "        elif dataStar[i] < 3 :\n",
    "            yearly[dataYear[i]][1] +=1\n",
    "        else:\n",
    "            yearly[dataYear[i]][2] +=1\n",
    "    else:\n",
    "        yearly[dataYear[i]] = [0,0,0]\n",
    "        if dataStar[i] > 3:\n",
    "            yearly[dataYear[i]][0] +=1\n",
    "        elif dataStar[i] < 3 :\n",
    "            yearly[dataYear[i]][1] +=1\n",
    "        else:\n",
    "            yearly[dataYear[i]][2] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef14861",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e25247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in yearly:\n",
    "    sum = 0\n",
    "    for j in yearly[i]:\n",
    "        sum+=j\n",
    "    yearly[i][0]/=sum\n",
    "    yearly[i][1]/=sum    \n",
    "    yearly[i][2]/=sum\n",
    "yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d30db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Satisfied', 'Unsatisfied', 'Neutral']\n",
    "def showPie(i):\n",
    "    print(\"Statistics Observed in the Year \", i)\n",
    "    plt.pie(yearly[i], labels = labels, autopct='%1.2f%%' ,normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88545e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "showPie(2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15287bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "showPie(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4382d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "showPie(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f13ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "showPie(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b14af",
   "metadata": {},
   "outputs": [],
   "source": [
    "showPie(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "showPie(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c37746",
   "metadata": {},
   "outputs": [],
   "source": [
    "showPie(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f96acab",
   "metadata": {},
   "source": [
    "# Show Variation in the Sentiment throughout the Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a35328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showperYear(j):\n",
    "    posperYear = []\n",
    "#     j = 0\n",
    "    for i in yearly:\n",
    "        posperYear.append(yearly[i][j])\n",
    "    return posperYear\n",
    "\n",
    "\n",
    "def sentimentPerYear(j):\n",
    "    plt.plot(yearLabels, showperYear(j))\n",
    "    plt.xlabel(\"Variation Over the years\")\n",
    "    plt.ylabel(\"Sentiment towards the network\")\n",
    "    plt.show()\n",
    "    \n",
    "def showAllSentimentPerYear():\n",
    "    plt.plot(yearLabels, showperYear(0), label='Satisfied')\n",
    "    plt.plot(yearLabels, showperYear(1), label='Unsatisfied')\n",
    "    plt.plot(yearLabels, showperYear(2), label='Neutral')\n",
    "    plt.xlabel(\"Variation Over the years\")\n",
    "    plt.ylabel(\"Sentiment towards the network\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45fefe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "showAllSentimentPerYear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84814c31",
   "metadata": {},
   "source": [
    "# State Wise Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "myddata = pd.read_csv(\"Outlier_removed.csv\")\n",
    "# mydata.dropna(subset=[\"State\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata['State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "myddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82443b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "encoder=ce.OneHotEncoder(cols='State',handle_unknown='return_nan',return_df=True,use_cat_names=True)\n",
    "\n",
    "\n",
    "data2=myddata[\"State\"]\n",
    "data1 = mydata[\"VaderAnalysis\"]\n",
    "encoder=ce.HashingEncoder(cols='State',n_components=10)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d270ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee036833",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = list(data2)\n",
    "State_dictionary={}\n",
    "i = 0 \n",
    "for row in data2:\n",
    "    if row in State_dictionary :\n",
    "        if data1[i] == 1:\n",
    "            State_dictionary[row][0] += 1\n",
    "        elif data1[i] == -1 :\n",
    "            State_dictionary[row][1] += 1\n",
    "        else :\n",
    "            State_dictionary[row][2] += 1\n",
    "    else:\n",
    "        State_dictionary[row] = [0,0,0]\n",
    "        if data1[i] == 1:\n",
    "            State_dictionary[row][0] += 1\n",
    "        elif data1[i] == -1 :\n",
    "            State_dictionary[row][1] += 1\n",
    "        else :\n",
    "            State_dictionary[row][2] += 1\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "State_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d366612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e543bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in State_dictionary:\n",
    "    sum = 0\n",
    "    temp = State_dictionary[key]\n",
    "    for j in State_dictionary[key]:\n",
    "        sum+=int(j)\n",
    "    State_dictionary[key] = [ float(i/sum)*100 for i in temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [15, 15]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.xlim(0, 120)\n",
    "for x in State_dictionary:\n",
    "    y = State_dictionary[x]\n",
    "    plt.barh(x, y[0], color='c' , label='Satisfied')\n",
    "    plt.barh(x, y[1], left=y[0], color='r',label='Unsatisfied')\n",
    "    plt.barh(x, y[2], left=y[0]+y[1], color='g',label='Neutral')\n",
    "plt.title(\"State vs Probability of Satisfaction\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Sates\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32469a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
